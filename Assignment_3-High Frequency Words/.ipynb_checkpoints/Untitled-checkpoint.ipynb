{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I am a huge coffee nerd, and searched for a dataset that contained coffee reviews so I could drill into it. This set is named the coffee reviews dataset and can be found here:\n",
    "https://www.kaggle.com/datasets/schmoyote/coffee-reviews-dataset\n",
    "\n",
    "In each item there is a review of a blend of coffee with 3 distinct reviews.\n",
    "\n",
    "I'm first going to prepare my enviroment, and tee up som basic spaCy and NLTK data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 2000000  #Increase the maximum length limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially the nltk is a fantastic natural langauge tool kit. We're going to use a tokenizer. A tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "\n",
    "In addition, were going to extract the stop words. A stop word is a frequently used word (like \"the,\" \"a,\" \"an,\" or \"in\"), and adds no relative meaning to the analysis were going to do. \n",
    "\n",
    "From here, were going to load in the CSV, extract the 3 reviews per coffee, and transform them into a singular dataframe, and save that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Evaluated as espresso. Sweet-toned, deeply ric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evaluated as espresso. Sweetly tart, floral-to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crisply sweet, cocoa-toned. Lemon blossom, roa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delicate, sweetly spice-toned. Pink peppercorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deeply sweet, subtly pungent. Honey, pear, tan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6280</th>\n",
       "      <td>A quietly confident, sweetly nut-toned Guatema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6281</th>\n",
       "      <td>A deeply floral, richly chocolaty Guatemala cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6282</th>\n",
       "      <td>A bright, balanced, juicy Guatemala cup driven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6283</th>\n",
       "      <td>Balanced, bright, invigoratingly crisp, with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>This single-farm Guatemala espresso displays t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6285 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description\n",
       "0     Evaluated as espresso. Sweet-toned, deeply ric...\n",
       "1     Evaluated as espresso. Sweetly tart, floral-to...\n",
       "2     Crisply sweet, cocoa-toned. Lemon blossom, roa...\n",
       "3     Delicate, sweetly spice-toned. Pink peppercorn...\n",
       "4     Deeply sweet, subtly pungent. Honey, pear, tan...\n",
       "...                                                 ...\n",
       "6280  A quietly confident, sweetly nut-toned Guatema...\n",
       "6281  A deeply floral, richly chocolaty Guatemala cu...\n",
       "6282  A bright, balanced, juicy Guatemala cup driven...\n",
       "6283  Balanced, bright, invigoratingly crisp, with t...\n",
       "6284  This single-farm Guatemala espresso displays t...\n",
       "\n",
       "[6285 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the CSV file\n",
    "file_path = 'coffee_analysis.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Extract reviews\n",
    "desc_df = df[['desc_1', 'desc_2', 'desc_3']]\n",
    "\n",
    "#Transform the reviews from one row with three reviews to three rows with 1 review (melt it)\n",
    "melted_desc_df = desc_df.melt(var_name='description_type', value_name='description').drop(columns='description_type')\n",
    "melted_desc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a corpus file so we ideally never have to do this again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the data\n",
    "melted_desc_df = melted_desc_df.dropna()\n",
    "#Turn it into a list, so that we can write it out. \n",
    "corpus = melted_desc_df['description'].tolist()\n",
    "\n",
    "#save the corpus\n",
    "corpus_file_path = 'descriptions_corpus.txt'\n",
    "with open(corpus_file_path, 'w', encoding='utf-8') as file:\n",
    "    for description in corpus:\n",
    "        file.write(description + '\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Descriptions': 6283,\n",
       " 'Average Length': 297.3931243036766,\n",
       " 'Longest Description': 'This exceptional coffee was selected as the No. 24 coffee on\\xa0Coffee Review’s\\xa0list of the Top 30 Coffees of 2018.\\xa0 This coffee tied for the highest rating in a tasting of natural-processed single-origin espressos for Coffee Review’s August 2018 tasting report. With its generally elongated beans and distinctive floral and crisp, often chocolaty cup, the Gesha variety of Arabica continues to distinguish itself as one of the world’s rarest and most unique coffees. Although the Gesha originated in Ethiopia, it was “discovered” by the coffee world in 2004 growing in Boquete, Panama, and Panama continues to dominate the expanding world of Gesha. This particular version, however, is the outcome of efforts to commercialize Gesha in the region from which it originally came. It was grown in western Ethiopia by farmers Adam and Rachel Overton and their indigenous Meanit culture collaborators from seed selected from wild trees in the nearby Gori Gesha forest. This is a dry-processed or “natural” version, meaning the beans were dried inside the fruit rather than after the soft fruit residue has been removed, as is the case with wet-processed or “washed” coffees. Small Eyes Café is a small, family-owned coffee shop in Yilan, Taiwan, specializing in artisanally roasted specialty coffee and homemade desserts. Visit https://www.facebook.com/Smalleyescafe for more information.',\n",
       " 'Length of Longest Description': 1380,\n",
       " 'Shortest Description': 'A plush, smooth roast-touched Kona.',\n",
       " 'Length of Shortest Description': 35}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_descriptions = len(corpus)\n",
    "lengths = [len(description) for description in corpus]\n",
    "average_length = sum(lengths) / total_descriptions\n",
    "longest_description = max(corpus, key=len)\n",
    "shortest_description = min(corpus, key=len)\n",
    "\n",
    "summary_statistics = {\n",
    "    'Total Descriptions': total_descriptions,\n",
    "    'Average Length': average_length,\n",
    "    'Longest Description': longest_description,\n",
    "    'Length of Longest Description': len(longest_description),\n",
    "    'Shortest Description': shortest_description,\n",
    "    'Length of Shortest Description': len(shortest_description)\n",
    "}\n",
    "summary_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(corpus[:20]) #were going to play with just 20 reviews, and then expand out to the whole set. This will help with runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a single text from all descriptions for NLP processing\n",
    "#all_text = \" \".join(corpus)\n",
    "#all_text = \" \".join(corpus[:20])\n",
    "\n",
    "#Tokenize\n",
    "tokens = word_tokenize(all_text.lower())\n",
    "\n",
    "#Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Descriptions</th>\n",
       "      <td>6283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Length</th>\n",
       "      <td>297.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longest Description</th>\n",
       "      <td>This exceptional coffee was selected as the No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length of Longest Description</th>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shortest Description</th>\n",
       "      <td>A plush, smooth roast-touched Kona.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length of Shortest Description</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexical Diversity</th>\n",
       "      <td>0.335484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Common Words</th>\n",
       "      <td>[(finish, 21), (aroma, 20), (cup, 20), (mouthf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Named Entities</th>\n",
       "      <td>[((three, CARDINAL), 4), ((Sweet, PERSON), 3),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            Value\n",
       "Total Descriptions                                                           6283\n",
       "Average Length                                                            297.393\n",
       "Longest Description             This exceptional coffee was selected as the No...\n",
       "Length of Longest Description                                                1380\n",
       "Shortest Description                          A plush, smooth roast-touched Kona.\n",
       "Length of Shortest Description                                                 35\n",
       "Lexical Diversity                                                        0.335484\n",
       "Most Common Words               [(finish, 21), (aroma, 20), (cup, 20), (mouthf...\n",
       "Named Entities                  [((three, CARDINAL), 4), ((Sweet, PERSON), 3),..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate how diverse the review is\n",
    "lexical_diversity = len(set(filtered_tokens)) / len(filtered_tokens)\n",
    "\n",
    "#Calculate 200 most common words\n",
    "word_freq = Counter(filtered_tokens)\n",
    "most_common_words = word_freq.most_common(200)\n",
    "\n",
    "#Use spaCy for NER\n",
    "doc = nlp(all_text)\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "entity_counter = Counter(entities)\n",
    "\n",
    "#Get interesting stuff\n",
    "summary_statistics = {\n",
    "    'Total Descriptions': total_descriptions,\n",
    "    'Average Length': average_length,\n",
    "    'Longest Description': longest_description,\n",
    "    'Length of Longest Description': len(longest_description),\n",
    "    'Shortest Description': shortest_description,\n",
    "    'Length of Shortest Description': len(shortest_description),\n",
    "    'Lexical Diversity': lexical_diversity,\n",
    "    'Most Common Words': most_common_words,\n",
    "    'Named Entities': entity_counter.most_common(10) #I wanted to see if there was any fun named entities\n",
    "}\n",
    "\n",
    "summary_statistics_df = pd.DataFrame.from_dict(summary_statistics, orient='index', columns=['Value'])\n",
    "summary_statistics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER doesn't seem to be working too well, but its rather interesting to see that finish and Aroma are pretty common words (at least among the 20 supercut) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34,\n",
       " ['finish',\n",
       "  'aroma',\n",
       "  'cup',\n",
       "  'mouthfeel',\n",
       "  'structure',\n",
       "  'chocolate',\n",
       "  'long',\n",
       "  'sweet',\n",
       "  'short',\n",
       "  'sweetly',\n",
       "  'notes',\n",
       "  'lemon',\n",
       "  'delicate',\n",
       "  'dark',\n",
       "  'zest',\n",
       "  'roasted',\n",
       "  'cacao',\n",
       "  'nib',\n",
       "  'peppercorn',\n",
       "  'floral',\n",
       "  'rich',\n",
       "  'resonant',\n",
       "  'tart',\n",
       "  'dried',\n",
       "  'gently',\n",
       "  'crisply',\n",
       "  'blossom',\n",
       "  'crisp',\n",
       "  'balanced',\n",
       "  'plush',\n",
       "  'syrupy',\n",
       "  'apricot',\n",
       "  'juicy',\n",
       "  'hints'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word_freq = word_freq.most_common()\n",
    "total_words = sum(word_freq.values())\n",
    "cumulative_count = 0\n",
    "unique_word_count = 0\n",
    "words_that_represent_half = []\n",
    "for word, freq in sorted_word_freq:\n",
    "    cumulative_count += freq\n",
    "    unique_word_count += 1\n",
    "    words_that_represent_half.append(word)\n",
    "    if cumulative_count >= total_words / 2:\n",
    "        break\n",
    "\n",
    "# Display the result\n",
    "unique_word_count, words_that_represent_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
